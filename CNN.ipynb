{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language Image Classification with a Convolutional Neural Network\n",
    "By Jens Lichter, Quentin Seifert and Anton Thielmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load recquired packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf  # tensorflow 2.0\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import seaborn\n",
    "import os\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some values that can be custmized by the user. They are written in all caps so the user can easily see at what points these values play a role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = 0.2\n",
    "INIT_STDDEV = 0.01\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 100\n",
    "KEEP_PROB = 0.7\n",
    "ROTATE = True\n",
    "VERTICAL = True\n",
    "BRIGHT = True\n",
    "MIXL1L2 = 0\n",
    "LAMBDA = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Make sure the paths are correct!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv('../sign-language-mnist/sign_mnist_train.csv')\n",
    "test = pd.read_csv('../sign-language-mnist/sign_mnist_test.csv')\n",
    "\n",
    "# dictonary for numbers of the labels\n",
    "un_labels = np.unique(train.iloc[:, 0])\n",
    "letters = np.delete(np.array(list(string.ascii_uppercase[0:25])), 9)\n",
    "s = pd.Series(letters, index=un_labels)\n",
    "dict_letters = s.to_dict()\n",
    "\n",
    "# dictionary for prediction -> delete entry with j\n",
    "s = pd.Series(letters)\n",
    "dict_pred = s.to_dict()\n",
    "\n",
    "# Since our target variable are in categorical(nomial) - binarize the labels\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_train = label_binarizer.fit_transform(train['label'].values)\n",
    "y_test = label_binarizer.fit_transform(test['label'].values)\n",
    "\n",
    "# drop the labels from training dataset - first column\n",
    "train.drop('label', axis=1, inplace=True)\n",
    "test.drop('label', axis=1, inplace=True)\n",
    "\n",
    "# Reshape the images\n",
    "x_train = train.values\n",
    "x_test = test.values\n",
    "\n",
    "\n",
    "# split training data to training and validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=VAL_SIZE, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation(data_x, data_y, angle, size):\n",
    "    rotate = ImageDataGenerator(rotation_range=angle)\n",
    "    rotate.fit(data_x)\n",
    "    for x_rot, y_rot in rotate.flow(data_x, data_y, batch_size=size, shuffle=True):\n",
    "        break\n",
    "    return x_rot, y_rot\n",
    "\n",
    "\n",
    "def vertical_shift(data_x, data_y, range, size):\n",
    "    vertical = ImageDataGenerator(width_shift_range=range)\n",
    "    vertical.fit(data_x)\n",
    "    for x_vert, y_vert in vertical.flow(data_x, data_y, batch_size=size, shuffle=True):\n",
    "        break\n",
    "    return x_vert, y_vert\n",
    "\n",
    "def brightness_change(data_x, data_y, range, size):\n",
    "    brightness = ImageDataGenerator(brightness_range=range)\n",
    "    brightness.fit(data_x)\n",
    "    for x_bright, y_bright in brightness.flow(data_x, data_y, batch_size=size, shuffle=True):\n",
    "        break\n",
    "    return x_bright, y_bright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the according arguments are set to `True` the following cell will augment the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ROTATE==True:\n",
    "    #reshape images for rotation function\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    #rotate images and append to complete data\n",
    "    from data_augmentation import rotation\n",
    "    images_new, labels_new = rotation(x_train, y_train, angle=30, size=2000)\n",
    "    x_train = np.concatenate((x_train, images_new), axis=0)\n",
    "    y_train = np.concatenate((y_train, labels_new), axis=0)\n",
    "    #reshape images for test split\n",
    "    x_train = np.array([i.flatten() for i in x_train])\n",
    "\n",
    "if VERTICAL==True:\n",
    "    #reshape images into format for vertical_shift functionx_train, y_train\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    #shift images vertically and append to complete data\n",
    "    from data_augmentation import vertical_shift\n",
    "    images_new, labels_new = vertical_shift(x_train, y_train, range=[-5,5], size=2000)\n",
    "    x_train = np.concatenate((x_train, images_new), axis=0)\n",
    "    y_train = np.concatenate((y_train, labels_new), axis=0)\n",
    "    #reshape images for test split\n",
    "    x_train = np.array([i.flatten() for i in x_train])\n",
    "\n",
    "if BRIGHT==True:\n",
    "    #reshape images into format for vertical_shift functionx_train, y_train\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    #shift images vertically and append to complete data\n",
    "    from data_augmentation import brightness_change\n",
    "    images_new, labels_new = brightness_change(x_train, y_train, range=[0.2, 0.9], size=2000)\n",
    "    x_train = np.concatenate((x_train, images_new), axis=0)\n",
    "    y_train = np.concatenate((y_train, labels_new), axis=0)\n",
    "    #reshape images for test split\n",
    "    x_train = np.array([i.flatten() for i in x_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define helper functions in order to make the construction of our network more comprehensible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights for convolutional layers - initialized randomly with truncated normal\n",
    "def weight_variable(shape):\n",
    "    initial = tf.random.truncated_normal(shape, stddev=INIT_STDDEV)\n",
    "    return(tf.Variable(initial))\n",
    "\n",
    "# bias in convolutional layers\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return(tf.Variable(initial))\n",
    "\n",
    "# specify convolution we are using (full convolution)\n",
    "def conv2d(x, W):\n",
    "    return(tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME'))\n",
    "\n",
    "# max pool, using 2x2 batches of the feature map\n",
    "def max_pool_2x2(x):\n",
    "    return(tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME'))\n",
    "\n",
    "# linear convolution with bias, followed by ReLU nonlinearity\n",
    "def conv_layer(input, shape):\n",
    "    W = weight_variable(shape)\n",
    "    b = bias_variable([shape[3]])\n",
    "    return([tf.nn.relu(conv2d(input, W) + b), W])\n",
    "\n",
    "# standard fully connected layer with bias\n",
    "def full_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = weight_variable([in_size, size])\n",
    "    b = bias_variable([size])\n",
    "    return([tf.matmul(input, W) + b, W])\n",
    "\n",
    "# define own next_batch function from MNIST\n",
    "def get_next_batch(x, y, start, end):\n",
    "    x_batch = x[start:end]\n",
    "    y_batch = y[start:end]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable eager execution\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Define Placeholders for images, labels and keep prob\n",
    "x = tf.compat.v1.placeholder(tf.float32, shape=[None, 784], name='x')\n",
    "y_ = tf.compat.v1.placeholder(tf.float32, shape=[None, 24], name='y_')\n",
    "keep_prob = tf.compat.v1.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# reshape image data into 2D image format with size 28x28x1, 28x28 pixels in one channel\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# two layers of convolution and pooling, using 16 and 32 filters respectively\n",
    "conv1, weights_1 = conv_layer(x_image, shape=[3, 3, 1, 16])\n",
    "conv1_pool = max_pool_2x2(conv1)\n",
    "conv1_pool = tf.compat.v1.nn.dropout(conv1_pool, rate=1-keep_prob)\n",
    "\n",
    "conv2, weights_2 = conv_layer(conv1_pool, shape=[3, 3, 16, 32])\n",
    "conv2_pool = max_pool_2x2(conv2)\n",
    "conv2_pool = tf.compat.v1.nn.dropout(conv2_pool, rate=1-keep_prob)\n",
    "\n",
    "# fully connected layer, activate with relu\n",
    "conv1_flat = tf.reshape(conv2_pool, [-1, 7*7*32])\n",
    "full_0, weights_3 = full_layer(conv1_flat, 256)\n",
    "full_1 = tf.nn.relu(full_0)\n",
    "\n",
    "# rate set to 1-keep_prob\n",
    "full1_drop = tf.compat.v1.nn.dropout(full_1, rate=1 - keep_prob)\n",
    "\n",
    "# output = fully connected layer with 24 units(labels of handsigns)\n",
    "y_conv, weights_4 = full_layer(full1_drop, 24)\n",
    "y_pred = tf.argmax(y_conv, 1, name='y_pred')\n",
    "\n",
    "# calculate loss by using softmax on logit model and apply cross entropy\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_))\n",
    "\n",
    "# add regularisation penalties\n",
    "l1 = tf.reduce_sum(tf.abs(weights_1)) + tf.reduce_sum(tf.abs(weights_2)) \\\n",
    "     + tf.reduce_sum(tf.abs(weights_3)) + tf.reduce_sum(tf.abs(weights_4))\n",
    "l2 = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) \\\n",
    "     + tf.nn.l2_loss(weights_3) + tf.nn.l2_loss(weights_4)\n",
    "shrinkage = tf.reduce_mean(cross_entropy + MIXL1L2 * LAMBDA * l1 + (1 - MIXL1L2) * LAMBDA * l2)\n",
    "\n",
    "# optimization with Adams optimizer\n",
    "train_step = tf.compat.v1.train.AdamOptimizer(1e-4).minimize(shrinkage)\n",
    "\n",
    "# predict values and get accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# create model saver\n",
    "saver = tf.compat.v1.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-335d35dd9e23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# start session and run model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# initialize weights and biases and set iteration length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\DLA-Sign-Language\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m   1583\u001b[0m           \u001b[0mprotocol\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mconfiguration\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m     \"\"\"\n\u001b[1;32m-> 1585\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1586\u001b[0m     \u001b[1;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1587\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\DLA-Sign-Language\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m    697\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m       \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version"
     ]
    }
   ],
   "source": [
    "# start session and run model\n",
    "with tf.compat.v1.Session() as sess:\n",
    "\n",
    "    # initialize weights and biases and set iteration length\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    num_tr_iter = int(len(y_train) / BATCH_SIZE)\n",
    "    global_step = 0\n",
    "\n",
    "    # loop through epochs\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Training epoch:  {epoch + 1}\")\n",
    "\n",
    "        # loop through iterations while calculating loss and accuracy for each\n",
    "        for i in range(num_tr_iter):\n",
    "            global_step += 1\n",
    "            start = i * BATCH_SIZE\n",
    "            end = (i + 1) * BATCH_SIZE\n",
    "            batch_xs, batch_ys = get_next_batch(x_train, y_train, start, end)\n",
    "\n",
    "            sess.run(train_step, feed_dict={x: batch_xs,\n",
    "                                            y_: batch_ys,\n",
    "                                            keep_prob: KEEP_PROB})\n",
    "\n",
    "            # after 100 iterations calculate and display the batch loss and accuracy\n",
    "            if i % 100 == 0:\n",
    "                loss_batch, acc_batch = sess.run([cross_entropy, accuracy], feed_dict={x: batch_xs,\n",
    "                                                                                      y_: batch_ys,\n",
    "                                                                                      keep_prob: 1.0})\n",
    "\n",
    "\n",
    "                print(f\"iter {i:3d}:\\t Loss={loss_batch:.3f},\\tTraining Accuracy={acc_batch:.5%}\")\n",
    "\n",
    "        # Run validation after every epoch\n",
    "        loss_valid, acc_valid = sess.run([cross_entropy, accuracy], feed_dict={x: x_val,\n",
    "                                                                      y_: y_val,\n",
    "                                                                      keep_prob: KEEP_PROB})\n",
    "        print('---------------------------------------------------------')\n",
    "        print(f\"Epoch: {epoch + 1}, validation loss: {loss_valid:.3f}, validation accuracy: {acc_valid:.5%}\")\n",
    "        print('---------------------------------------------------------')\n",
    "\n",
    "        # implement early stopping\n",
    "        if epoch == 0:\n",
    "            if not os.path.exists('./trained_model'):\n",
    "                os.makedirs('./trained_model')\n",
    "            saver.save(sess, './trained_model/model')\n",
    "            best_loss_valid = loss_valid\n",
    "            continue\n",
    "\n",
    "        if loss_valid < best_loss_valid:\n",
    "            best_loss_valid = loss_valid\n",
    "            saver.save(sess, './trained_model/model')\n",
    "\n",
    "        if (loss_valid / best_loss_valid - 1) * 100 > 4:\n",
    "            saver.restore(sess, './trained_model/model')\n",
    "            print('---------------------------------------------------------')\n",
    "            print('\\t \\t \\t STOPPING EARLY')\n",
    "            print('---------------------------------------------------------')\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # test model performance on test data\n",
    "    test_accuracy = np.mean([sess.run(accuracy,\n",
    "                                      feed_dict={x: x_test,\n",
    "                                                 y_: y_test,\n",
    "                                                 keep_prob: 1.0})])\n",
    "\n",
    "print(f\"test accuracy: {test_accuracy:.5%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write functions for plotting correct and incorrect examples\n",
    "def plot_images(images, cls_true, cls_pred=None, title=None):\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(6, 6))\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(28, 28), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        ax_title = f\"True:{dict_pred[cls_true[i]]}  -  Pred: {dict_pred[cls_pred[i]]}\"\n",
    "\n",
    "        ax.set_title(ax_title)\n",
    "\n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    if title:\n",
    "        plt.suptitle(title, size=20)\n",
    "    plt.show(block=False)\n",
    "    \n",
    "def plot_example_errors(images, cls_true, cls_pred, title=None):\n",
    "\n",
    "    # retrieve incorrectly classified pics\n",
    "    incorrect = np.logical_not(np.equal(cls_pred, cls_true))\n",
    "\n",
    "    incorrect_images = images[incorrect]\n",
    "\n",
    "    # Get the true and predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "    cls_true = cls_true[incorrect]\n",
    "\n",
    "    # Plot the first 4 images.\n",
    "    plot_images(images=incorrect_images,\n",
    "                cls_true=cls_true,\n",
    "                cls_pred=cls_pred,\n",
    "                title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of correctly and misclassified images\n",
    "We use our trained network to predict the test dataset. The following cell shows some examples of correctly and incorrectly classified images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    saver.restore(sess, './trained_model/model')\n",
    "    cls_pred = sess.run(y_pred, feed_dict={x: x_test, y_: y_test, keep_prob: 1.0})\n",
    "    \n",
    "y_lab = []\n",
    "\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_lab.append(np.argmax(y_test[i,]))\n",
    "\n",
    "y_lab = np.array(y_lab)\n",
    "# plot correct and incorrect predicted examples\n",
    "cls_true = np.argmax(y_test, axis=1)\n",
    "plot_images(x_test, cls_true, cls_pred, title='Correct Examples')\n",
    "plot_example_errors(x_test, cls_true, cls_pred, title='Misclassified Examples')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "Using the predictions from before, we create a confusion matrix. As the name already suggest, the confusion matrix illustrates which letters are 'confused' for others. Given our high test accuracy, it is not surprising that all letters are classified correctly most of the times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = tf.math.confusion_matrix(y_lab, cls_pred,\n",
    "                                     num_classes=24, weights=None, dtype=tf.dtypes.int32, name=None)\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    saver.restore(sess, './trained_model/model')\n",
    "    x = sess.run(confusion)\n",
    "    \n",
    "x = np.around(x.astype('float') / x.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "x_df = pd.DataFrame(x,\n",
    "                    index=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n",
    "                           'S', 'T', 'U', 'V', 'W', 'X', 'Y'],\n",
    "                    columns=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n",
    "                             'S', 'T', 'U', 'V', 'W', 'X', 'Y'])\n",
    "\n",
    "figure = plt.figure(figsize=(24, 24))\n",
    "seaborn.heatmap(x_df, annot=True, cmap='cividis')\n",
    "#plt.tight_layout()\n",
    "plt.ylabel('True label');\n",
    "plt.xlabel('Predicted label');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
